Functional requirement:
1.) uploading
2.) dowloading
3.)Sync between different devices
4.)Notifications

Non Functional requirement:
1.)High availability
2.)High consistency
3.)Scaling purpose will be handled after getting an overview of no. of devices in the cluster

Clients:
mobile,desktop

Devices in cluster
1.) 100 million devices(1 million active daily)
2.) 1 file per day per device
3.)avg file size 5mb, max 15 gigs

Math=1 million* 15 gigabyte=15 petabyte
QPS=1 million/24*3600=11
peak QPS=15
traffic=1million*5mb=5tb



Components:

At first I will design a setup for our client side server itself.
So my clients desktop app,website will be having following 4 important Components:
1.)Filewatcher:- This will keep an eye on whether there is some file uploaded currently or not.This notifies the chunker and 
indexer that there is one new file that has arrived in the required folder

2.) Chunker:-This will divide the files into multiple chunks and uploads it to the cloud say azure which will act here as an storage 
service. Chunker does this uniquely by assigning a hashcode to each and every chunks that it uploads.
3.)Indexer:-This will craete indexing or say will prepare the metadata. The hashcode created when chunker was uploading the 
file to cloud and the url that we will get in return will be stored here for record purpose inside the inetrnal DB
4.) Internal DB:- this will store the data related to the file loading. say the hashcode and url.

5.) Indexer:- This component will also notify a mesage queue(rabbit mq,kafka) that there is one new file that has been added.
6.) Message queue:- This message queue will be event driven service and will 
a sync service which will store the chuncks hashcode and all the details related to it in the database or cache and the notify 
the message queue back to notify the other devices in the cluster to keep them up to date.



1.) design pattern:
To take the file decide that what is the format of the files, we will use adapter design pattern

Now file-watcher will tell us that there is a type of file of specific type that has arrived and will hand it over to chunker which will
then make partition of the huge file and store the chunks metadata and urls where it will be stored in the cloud in an internal DB.






